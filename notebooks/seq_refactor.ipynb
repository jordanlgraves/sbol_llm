{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "from Bio import Align\n",
    "import sbol2\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "# import time\n",
    "\n",
    "def load_target_file(target_file):\n",
    "    logger = logging.getLogger('synbict')\n",
    "\n",
    "    if target_file.endswith('.xml') or target_file.endswith('.sbol'):\n",
    "        return load_sbol(target_file)\n",
    "    elif (target_file.endswith('.gb')\n",
    "            or target_file.endswith('.genbank')\n",
    "            or target_file.endswith('.fasta')\n",
    "            or target_file.endswith('.faa')\n",
    "            or target_file.endswith('.fa')\n",
    "            or target_file.endswith('.fas')\n",
    "            or target_file.endswith('.fsa')):\n",
    "        return load_non_sbol(target_file)\n",
    "    else:\n",
    "        logger.error('Extension of target file %s is unrecognized.', target_file)\n",
    "\n",
    "        return None\n",
    "\n",
    "# Set up the not found error for catching\n",
    "try:\n",
    "    # SBOLError is in the native python module\n",
    "    NotFoundError = sbol2.SBOLError\n",
    "except NameError:\n",
    "    # The swig wrapper raises RuntimeError on not found\n",
    "    NotFoundError = RuntimeError\n",
    "\n",
    "# Set up the not unique error for catching\n",
    "try:\n",
    "    # SBOLError is in the native python module\n",
    "    NotUniqueError = sbol2.SBOLError\n",
    "except NameError:\n",
    "    # The swig wrapper raises RuntimeError on not unique\n",
    "    NotUniqueError = RuntimeError\n",
    "\n",
    "def is_sbol_not_found(exc):\n",
    "    return (exc.error_code() == sbol2.SBOLErrorCode.SBOL_ERROR_NOT_FOUND\n",
    "        or exc.error_code() == sbol2.SBOLErrorCode.NOT_FOUND_ERROR)\n",
    "\n",
    "def load_sbol(sbol_file):\n",
    "    logger = logging.getLogger('synbict')\n",
    "\n",
    "    logger.info('Loading %s', sbol_file)\n",
    "\n",
    "    doc = sbol2.Document()\n",
    "    doc.read(sbol_file)\n",
    "\n",
    "    doc.name = sbol_file\n",
    "\n",
    "    doc.addNamespace('http://purl.org/dc/elements/1.1/', 'dc')\n",
    "    doc.addNamespace('http://wiki.synbiohub.org/wiki/Terms/igem#', 'igem')\n",
    "    doc.addNamespace('http://wiki.synbiohub.org/wiki/Terms/synbiohub#', 'sbh')\n",
    "    doc.addNamespace('http://sbolstandard.org/gff3#', 'gff3')\n",
    "    doc.addNamespace('http://cellocad.org/Terms/cello#', 'cello')\n",
    "\n",
    "    return doc\n",
    "\n",
    "def load_non_sbol(non_sbol_file):\n",
    "    logger = logging.getLogger('synbict')\n",
    "\n",
    "    logger.info('Loading %s', non_sbol_file)\n",
    "\n",
    "    conversion_request = {\n",
    "        'options': {\n",
    "            'language' : 'SBOL2',\n",
    "            'test_equality': False,\n",
    "            'check_uri_compliance': False,\n",
    "            'check_completeness': False,\n",
    "            'check_best_practices': False,\n",
    "            'fail_on_first_error': False,\n",
    "            'provide_detailed_stack_trace': False,\n",
    "            'subset_uri': '',\n",
    "            'uri_prefix': sbol2.getHomespace(),\n",
    "            'version': '1',\n",
    "            'insert_type': False,\n",
    "            'main_file_name': 'main file',\n",
    "            'diff_file_name': 'comparison file'\n",
    "        },\n",
    "        'return_file': True,\n",
    "        'main_file': open(non_sbol_file).read()\n",
    "    }\n",
    "\n",
    "    conversion_response = requests.post(\"https://validator.sbolstandard.org/validate/\", json=conversion_request)\n",
    "\n",
    "    response_dict = json.loads(conversion_response.content.decode('utf-8'))\n",
    "\n",
    "    doc = sbol2.Document()\n",
    "    doc.readString(response_dict['result'])\n",
    "\n",
    "    doc.name = non_sbol_file\n",
    "\n",
    "    doc.addNamespace('http://purl.org/dc/elements/1.1/', 'dc')\n",
    "    doc.addNamespace('http://wiki.synbiohub.org/wiki/Terms/igem#', 'igem')\n",
    "    doc.addNamespace('http://wiki.synbiohub.org/wiki/Terms/synbiohub#', 'sbh')\n",
    "\n",
    "    return doc\n",
    "\n",
    "class FeatureCurator():\n",
    "\n",
    "    def __init__(self, target_library, output_library=None):\n",
    "        self.target_library = target_library\n",
    "        self.output_library = output_library\n",
    "\n",
    "        self.logger = logging.getLogger('synbict')\n",
    "\n",
    "    def annotate_features(self, feature_annotater, min_target_length, in_place=False, complete_matches=False,\n",
    "                          strip_prefixes=[]):\n",
    "        # start_time = time.clock()\n",
    "\n",
    "        annotated_identities = feature_annotater.annotate(self.target_library, min_target_length, in_place,\n",
    "                                                          self.output_library, complete_matches, strip_prefixes)\n",
    "\n",
    "        # self.logger.info('Annotation Time: ' + str(time.clock() - start_time))\n",
    "\n",
    "        if self.output_library and len(self.output_library.docs) > 0:\n",
    "            added_features = self.output_library.update(False)\n",
    "        else:\n",
    "            added_features = self.target_library.update()\n",
    "\n",
    "        annotated_features = []\n",
    "        annotating_features = []\n",
    "\n",
    "        for added_feature in added_features:\n",
    "            if added_feature.identity in annotated_identities:\n",
    "                annotated_features.append(added_feature)\n",
    "            else:\n",
    "                annotating_features.append(added_feature)\n",
    "\n",
    "        return (annotated_features, annotating_features)\n",
    "\n",
    "    def prune_features(self, feature_pruner, cover_offset, min_target_length, target_features=[],\n",
    "            target_sub_features=[], delete_flat=False, auto_swap=False, ask_user=True):\n",
    "        if self.output_library and len(self.output_library.docs) > 0:\n",
    "            feature_pruner.prune(self.output_library, cover_offset, min_target_length,\n",
    "                                 ask_user=ask_user, delete_flat=delete_flat, target_features=target_features,\n",
    "                                 auto_swap=auto_swap, require_sequence=False)\n",
    "        else:\n",
    "            feature_pruner.prune(self.target_library, cover_offset, min_target_length,\n",
    "                                 ask_user=ask_user, delete_flat=delete_flat, target_features=target_features,\n",
    "                                 auto_swap=auto_swap)\n",
    "\n",
    "            feature_pruner.clean(self.target_library, target_features, target_sub_features)\n",
    "\n",
    "    def extend_features(self, feature_annotater, min_target_length, extension_threshold, strip_prefixes=[]):\n",
    "        # start_time = time.clock()\n",
    "\n",
    "        feature_annotater.extend_features_by_name(self.target_library,\n",
    "                                                  min_target_length,\n",
    "                                                  extension_threshold,\n",
    "                                                  strip_prefixes)\n",
    "\n",
    "        # self.logger.info('Extension Time: ' + str(time.clock() - start_time))\n",
    "\n",
    "class Feature():\n",
    "\n",
    "    SO_REGION = 'http://identifiers.org/so/SO:0000001'\n",
    "    SO_SEQUENCE_FEATURE = 'http://identifiers.org/so/SO:0000110'\n",
    "\n",
    "    GENERIC_ROLES = {\n",
    "        SO_REGION,\n",
    "        SO_SEQUENCE_FEATURE\n",
    "    }\n",
    "\n",
    "    def __init__(self, nucleotides, identity, roles, sub_identities=[], parent_identities=[]):\n",
    "        self.nucleotides = nucleotides\n",
    "        self.identity = identity\n",
    "        self.sub_identities = sub_identities\n",
    "        self.parent_identities = parent_identities\n",
    "        self.roles = set(roles)\n",
    "\n",
    "        self.logger = logging.getLogger('synbict')\n",
    "\n",
    "    def reverse_complement_nucleotides(self):\n",
    "        return str(Seq(self.nucleotides).reverse_complement())\n",
    "\n",
    "    @classmethod\n",
    "    def has_non_generic_role(cls, roles):\n",
    "        return len(roles.difference(cls.GENERIC_ROLES)) > 0\n",
    "\n",
    "    def is_non_generic(self):\n",
    "        return self.has_non_generic_role(self.roles)\n",
    "\n",
    "class FeatureLibrary():\n",
    "\n",
    "    def __init__(self, docs, require_sequence=True):\n",
    "        self.features = []\n",
    "        self.docs = docs\n",
    "\n",
    "        self.__updated_indices = set()\n",
    "        self.__feature_map = {}\n",
    "        self.__feature_dict = {}\n",
    "        self.__name_to_idents = {}\n",
    "\n",
    "        self.logger = logging.getLogger('synbict')\n",
    "\n",
    "        self.logger.info('Loading features')\n",
    "\n",
    "        for i in range(0, len(self.docs)):\n",
    "            self.__load_features(self.docs[i], i, require_sequence)\n",
    "\n",
    "    def update(self, require_sequence=True):\n",
    "        added_features = []\n",
    "\n",
    "        for i in range(0, len(self.docs)):\n",
    "            added_features.extend(self.__load_features(self.docs[i], i, require_sequence))\n",
    "\n",
    "        for added_feature in added_features:\n",
    "            self.__updated_indices.add(self.get_document_index(added_feature.identity))\n",
    "\n",
    "        return added_features\n",
    "\n",
    "    def get_updated_documents(self):\n",
    "        updated_docs = []\n",
    "\n",
    "        for updated_index in self.__updated_indices:\n",
    "            updated_docs.append(self.docs[updated_index])\n",
    "\n",
    "        return updated_docs\n",
    "\n",
    "    def get_non_updated_indices(self):\n",
    "        non_updated_indices = []\n",
    "\n",
    "        for i in range(0, len(self.docs)):\n",
    "            if i not in self.__updated_indices:\n",
    "                non_updated_indices.append(i)\n",
    "\n",
    "        return non_updated_indices\n",
    "\n",
    "    def __load_features(self, doc, doc_index, require_sequence=True):\n",
    "        loaded_features = []\n",
    "\n",
    "        comp_seq_identities = set()\n",
    "\n",
    "        for comp_definition in doc.componentDefinitions:\n",
    "            if sbol2.BIOPAX_DNA in comp_definition.types:\n",
    "                dna_seqs = self.get_DNA_sequences(comp_definition, doc)\n",
    "\n",
    "                for dna_seq in dna_seqs:\n",
    "                    comp_seq_identities.add(dna_seq.identity)\n",
    "\n",
    "                if comp_definition.identity not in self.__feature_map:\n",
    "                    sub_identities = []\n",
    "\n",
    "                    for sub_comp in comp_definition.components:\n",
    "                        sub_identities.append(sub_comp.definition)\n",
    "\n",
    "                    if len(dna_seqs) > 0:\n",
    "                        feature = Feature(dna_seqs[0].elements,\n",
    "                                          comp_definition.identity,\n",
    "                                          comp_definition.roles,\n",
    "                                          sub_identities,\n",
    "                                          comp_definition.wasDerivedFrom)\n",
    "\n",
    "                        loaded_features.append(feature)\n",
    "                        self.features.append(feature)\n",
    "\n",
    "                        self.__feature_map[comp_definition.identity] = doc_index\n",
    "                        self.__feature_dict[comp_definition.identity] = feature\n",
    "\n",
    "                        if comp_definition.name:\n",
    "                            if comp_definition.name not in self.__name_to_idents:\n",
    "                                self.__name_to_idents[comp_definition.name] = []\n",
    "\n",
    "                            self.__name_to_idents[comp_definition.name].append(comp_definition.identity)\n",
    "                    elif not require_sequence:\n",
    "                        feature = Feature('',\n",
    "                                          comp_definition.identity,\n",
    "                                          comp_definition.roles,\n",
    "                                          sub_identities,\n",
    "                                          comp_definition.wasDerivedFrom)\n",
    "\n",
    "                        loaded_features.append(feature)\n",
    "                        self.features.append(feature)\n",
    "\n",
    "                        self.__feature_map[comp_definition.identity] = doc_index\n",
    "                        self.__feature_dict[comp_definition.identity] = feature\n",
    "\n",
    "                        if comp_definition.name:\n",
    "                            if comp_definition.name not in self.__name_to_idents:\n",
    "                                self.__name_to_idents[comp_definition.name] = []\n",
    "\n",
    "                            self.__name_to_idents[comp_definition.name].append(comp_definition.identity)\n",
    "                    else:\n",
    "                        self.logger.warning('%s not loaded since its DNA sequence was not found', comp_definition.identity)\n",
    "\n",
    "        for seq in doc.sequences:\n",
    "            if seq.identity not in comp_seq_identities and seq.encoding == sbol2.SBOL_ENCODING_IUPAC:\n",
    "                seq_comp_definition = sbol2.ComponentDefinition(seq.displayId + '_comp', sbol2.BIOPAX_DNA, '1')\n",
    "                seq_comp_definition.sequences = [seq.identity]\n",
    "\n",
    "                try:\n",
    "                    doc.addComponentDefinition(seq_comp_definition)\n",
    "\n",
    "                    feature = Feature(seq.elements,\n",
    "                                      seq_comp_definition.identity,\n",
    "                                      [],\n",
    "                                      [],\n",
    "                                      [])\n",
    "\n",
    "                    loaded_features.append(feature)\n",
    "                    self.features.append(feature)\n",
    "\n",
    "                    self.__feature_map[seq_comp_definition.identity] = doc_index\n",
    "                    self.__feature_dict[seq_comp_definition.identity] = feature\n",
    "                except RuntimeError:\n",
    "                    self.logger.warning('Component could not be automatically generated for DNA sequence %s', seq.identity)\n",
    "                except NotUniqueError as exc:\n",
    "                    if exc.error_code() == sbol2.SBOLErrorCode.SBOL_ERROR_URI_NOT_UNIQUE:\n",
    "                        self.logger.warning('Component could not be automatically generated for DNA sequence %s', seq.identity)\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "        return loaded_features\n",
    "\n",
    "    def get_features(self, min_feature_length=0, children_only=False):\n",
    "        features = []\n",
    "\n",
    "        if children_only:\n",
    "            parent_identities = set()\n",
    "\n",
    "            for feature in self.features:\n",
    "                for parent_identity in feature.parent_identities:\n",
    "                    parent_identities.add(parent_identity)\n",
    "\n",
    "            for feature in self.features:\n",
    "                if (min_feature_length == 0 or len(feature.nucleotides) > min_feature_length) and feature.identity not in parent_identities:\n",
    "                    features.append(feature)\n",
    "        else:\n",
    "            for feature in self.features:\n",
    "                if min_feature_length == 0 or len(feature.nucleotides) > min_feature_length:\n",
    "                    features.append(feature)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def get_added_feature_identities(self):\n",
    "        added_feature_identities = set()\n",
    "\n",
    "        for doc in self.docs:\n",
    "            for comp_definition in doc.componentDefinitions:\n",
    "                if comp_definition.identity not in self.__feature_map:\n",
    "                    added_feature_identities.append(comp_definition.identity)\n",
    "\n",
    "        return added_feature_identities\n",
    "\n",
    "    def get_document(self, identity):\n",
    "        return self.docs[self.get_document_index(identity)]\n",
    "\n",
    "    def get_document_index(self, identity):\n",
    "        if identity in self.__feature_map:\n",
    "            return self.__feature_map[identity]\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def get_definition(self, identity):\n",
    "        return self.get_document(identity).getComponentDefinition(identity)\n",
    "\n",
    "    def get_definitions_by_name(self, name):\n",
    "        name_keys = []\n",
    "\n",
    "        if name in self.__name_to_idents:\n",
    "            name_keys.append(name)\n",
    "        else:\n",
    "            for other_name in self.__name_to_idents:\n",
    "                if name in other_name or other_name in name:\n",
    "                    name_keys.append(other_name)\n",
    "\n",
    "        definitions = []\n",
    "\n",
    "        for name_key in name_keys:\n",
    "            identities = self.__name_to_idents[name_key]\n",
    "\n",
    "            for identity in identities:\n",
    "                definitions.append(self.get_definition(identity))\n",
    "\n",
    "        return definitions\n",
    "\n",
    "    def has_feature(self, identity):\n",
    "        return identity in self.__feature_map\n",
    "\n",
    "    def get_feature(self, identity):\n",
    "        return self.__feature_dict[identity]\n",
    "\n",
    "    @classmethod\n",
    "    def get_DNA_sequences(cls, comp_definition, doc):\n",
    "        dna_seqs = []\n",
    "\n",
    "        for seq_URI in comp_definition.sequences:\n",
    "            try:\n",
    "                seq = doc.getSequence(seq_URI)\n",
    "            except RuntimeError:\n",
    "                seq = None\n",
    "            except NotFoundError as exc:\n",
    "                if is_sbol_not_found(exc):\n",
    "                    seq = None\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "            if seq and seq.encoding == sbol2.SBOL_ENCODING_IUPAC:\n",
    "                dna_seqs.append(seq)\n",
    "\n",
    "        return dna_seqs\n",
    "\n",
    "    @classmethod\n",
    "    def get_sequences(cls, comp_definition, doc):\n",
    "        seqs = []\n",
    "\n",
    "        for seq_URI in comp_definition.sequences:\n",
    "            try:\n",
    "                seq = doc.getSequence(seq_URI)\n",
    "            except RuntimeError:\n",
    "                seq = None\n",
    "            except NotFoundError as exc:\n",
    "                if is_sbol_not_found(exc):\n",
    "                    seq = None\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "            if seq:\n",
    "                seqs.append(seq)\n",
    "\n",
    "        return seqs\n",
    "\n",
    "    @classmethod\n",
    "    def copy_sequence(cls, seq, source_doc, sink_doc, import_namespace=False, strip_prefixes=[]):\n",
    "        if import_namespace:\n",
    "            namespace = '/'.join(seq.identity.split('/')[:-2])\n",
    "\n",
    "            if namespace == sbol2.getHomespace():\n",
    "                try:\n",
    "                    version = int(seq.version)\n",
    "                except (TypeError, ValueError):\n",
    "                    return None\n",
    "\n",
    "                try:\n",
    "                    seq_copy = seq.copy(sink_doc, namespace, str(version + 1))\n",
    "\n",
    "                except RuntimeError:\n",
    "                    return sink_doc.getSequence('/'.join([sbol2.getHomespace(), seq.displayId,\n",
    "                                                          str(version + 1)]))\n",
    "                except NotUniqueError as exc:\n",
    "                    if exc.error_code() == sbol2.SBOLErrorCode.SBOL_ERROR_URI_NOT_UNIQUE:\n",
    "                        return sink_doc.getSequence('/'.join([sbol2.getHomespace(), seq.displayId,\n",
    "                                                              str(version + 1)]))\n",
    "                    else:\n",
    "                        raise\n",
    "                    \n",
    "            else:\n",
    "                try:\n",
    "                    seq_copy = seq.copy(sink_doc, namespace, '1')\n",
    "                except RuntimeError:\n",
    "                    return sink_doc.getSequence('/'.join([sbol2.getHomespace(), seq.displayId, '1']))\n",
    "                except NotUniqueError as exc:\n",
    "                    if exc.error_code() == sbol2.SBOLErrorCode.SBOL_ERROR_URI_NOT_UNIQUE:\n",
    "                        return sink_doc.getSequence('/'.join([sbol2.getHomespace(), seq.displayId, '1']))\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "            cls.strip_origin_properties(seq_copy, strip_prefixes)\n",
    "        else:\n",
    "            try:\n",
    "                sink_doc.getSequence(seq.identity)\n",
    "                \n",
    "                return None\n",
    "            except RuntimeError:\n",
    "                seq_copy = seq.copy(sink_doc)\n",
    "            except NotFoundError as exc:\n",
    "                if is_sbol_not_found(exc):\n",
    "                    seq_copy = seq.copy(sink_doc)\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "        return seq_copy\n",
    "\n",
    "    @classmethod\n",
    "    def make_variant_definition(cls, doc, definition_copy):\n",
    "        doc.componentDefinitions.remove(definition_copy.identity)\n",
    "\n",
    "        variant_index = 1\n",
    "        unique_flag = False\n",
    "        \n",
    "        while not unique_flag:\n",
    "            variant_ID = '_'.join([definition_copy.displayId, 'v' + str(variant_index)])\n",
    "\n",
    "            split_identity = definition_copy.identity.split('/')\n",
    "            variant_identity = '/'.join(split_identity[:-2] + [variant_ID, split_identity[-1]])\n",
    "            variant_p_identity = '/'.join(split_identity[:-2] + [variant_ID])\n",
    "\n",
    "            original_identity = definition_copy.identity\n",
    "            original_ID = definition_copy.displayId\n",
    "            original_p_identity = definition_copy.persistentIdentity\n",
    "\n",
    "            definition_copy.identity = variant_identity\n",
    "            definition_copy.displayId = variant_ID\n",
    "            definition_copy.persistentIdentity = variant_p_identity\n",
    "\n",
    "            try:\n",
    "                doc.componentDefinitions.add(definition_copy)\n",
    "\n",
    "                unique_flag = True\n",
    "            except RuntimeError:\n",
    "                definition_copy.identity = original_identity\n",
    "                definition_copy.displayId = original_ID\n",
    "                definition_copy.persistentIdentity = original_p_identity\n",
    "\n",
    "                variant_index = variant_index + 1\n",
    "\n",
    "                unique_flag = False\n",
    "            except NotUniqueError as exc:\n",
    "                if exc.error_code() == sbol2.SBOLErrorCode.SBOL_ERROR_URI_NOT_UNIQUE:\n",
    "                    definition_copy.identity = original_identity\n",
    "                    definition_copy.displayId = original_ID\n",
    "                    definition_copy.persistentIdentity = original_p_identity\n",
    "\n",
    "                    variant_index = variant_index + 1\n",
    "\n",
    "                    unique_flag = False\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "    @classmethod\n",
    "    def make_variant_sequence(cls, doc, sequence_copy):\n",
    "        doc.sequences.remove(sequence_copy.identity)\n",
    "\n",
    "        variant_index = 1\n",
    "        unique_flag = False\n",
    "        \n",
    "        while not unique_flag:\n",
    "            variant_ID = '_'.join([sequence_copy.displayId, 'v' + str(variant_index)])\n",
    "\n",
    "            split_identity = sequence_copy.identity.split('/')\n",
    "            variant_identity = '/'.join(split_identity[:-2] + [variant_ID, split_identity[-1]])\n",
    "            variant_p_identity = '/'.join(split_identity[:-2] + [variant_ID])\n",
    "\n",
    "            original_identity = sequence_copy.identity\n",
    "            original_ID = sequence_copy.displayId\n",
    "            original_p_identity = sequence_copy.persistentIdentity\n",
    "\n",
    "            sequence_copy.identity = variant_identity\n",
    "            sequence_copy.displayId = variant_ID\n",
    "            sequence_copy.persistentIdentity = variant_p_identity\n",
    "\n",
    "            try:\n",
    "                doc.sequences.add(sequence_copy)\n",
    "\n",
    "                unique_flag = True\n",
    "            except RuntimeError:\n",
    "                sequence_copy.identity = original_identity\n",
    "                sequence_copy.displayId = original_ID\n",
    "\n",
    "                variant_index = variant_index + 1\n",
    "\n",
    "                unique_flag = False\n",
    "            except NotUniqueError as exc:\n",
    "                if exc.error_code() == sbol2.SBOLErrorCode.SBOL_ERROR_URI_NOT_UNIQUE:\n",
    "                    sequence_copy.identity = original_identity\n",
    "                    sequence_copy.displayId = original_ID\n",
    "\n",
    "                    variant_index = variant_index + 1\n",
    "\n",
    "                    unique_flag = False\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "    # prop.startswith('http://wiki.synbiohub.org/wiki/Terms/synbiohub#')\n",
    "    # prop.startswith('http://www.ncbi.nlm.nih.gov/genbank#')\n",
    "    # prop.startswith('http://sbols.org/genBankConversion#')\n",
    "\n",
    "    @classmethod\n",
    "    def strip_origin_properties(cls, sbol_obj, other_prefixes):\n",
    "        strip_props = []\n",
    "\n",
    "        origin_prefixes = ['http://purl.org/dc/terms/created',\n",
    "                           'http://purl.org/dc/terms/modified',\n",
    "                           'http://purl.org/dc/terms/creator']\n",
    "\n",
    "        strip_prefixes = tuple(origin_prefixes + other_prefixes)\n",
    "\n",
    "        for prop in sbol_obj.properties:\n",
    "            if prop.startswith(strip_prefixes):\n",
    "                strip_props.append(prop)\n",
    "        for strip_prop in strip_props:\n",
    "            del sbol_obj.properties[strip_prop]\n",
    "\n",
    "        sbol_obj.wasGeneratedBy = []\n",
    "\n",
    "    @classmethod\n",
    "    def copy_component_definition(cls, comp_definition, source_doc, sink_doc, import_namespace=False,\n",
    "                                  min_seq_length=0, import_sequences=False, seq_elements=None,\n",
    "                                  parent_definitions=[], parent_doc=None, make_variant=False,\n",
    "                                  shallow_copy=False, strip_prefixes=[]):\n",
    "        if sbol2.BIOPAX_DNA in comp_definition.types:\n",
    "            seqs = cls.get_DNA_sequences(comp_definition, source_doc)\n",
    "        else:\n",
    "            seqs = cls.get_sequences(comp_definition, source_doc)\n",
    "\n",
    "        if min_seq_length == 0 or (len(seqs) > 0 and len(seqs[0].elements) >= min_seq_length):\n",
    "            namespace = '/'.join(comp_definition.identity.split('/')[:-2])\n",
    "\n",
    "            if import_namespace:\n",
    "                if namespace == sbol2.getHomespace():\n",
    "                    try:\n",
    "                        version = int(comp_definition.version)\n",
    "                    except (TypeError, ValueError):\n",
    "                        return None\n",
    "\n",
    "                    try:\n",
    "                        definition_copy = comp_definition.copy(sink_doc, namespace, str(version + 1))\n",
    "                    except RuntimeError:\n",
    "                        return sink_doc.getComponentDefinition('/'.join([sbol2.getHomespace(),\n",
    "                                                                         comp_definition.displayId,\n",
    "                                                                         str(version + 1)]))\n",
    "                    except NotUniqueError as exc:\n",
    "                        if exc.error_code() == sbol2.SBOLErrorCode.SBOL_ERROR_URI_NOT_UNIQUE:\n",
    "                            return sink_doc.getComponentDefinition('/'.join([sbol2.getHomespace(),\n",
    "                                                                             comp_definition.displayId,\n",
    "                                                                             str(version + 1)]))\n",
    "                        else:\n",
    "                            raise\n",
    "                        \n",
    "                else:\n",
    "                    try:\n",
    "                        definition_copy = comp_definition.copy(sink_doc, namespace, '1')\n",
    "                    except RuntimeError:\n",
    "                        return sink_doc.getComponentDefinition('/'.join([sbol2.getHomespace(),\n",
    "                                                                         comp_definition.displayId, '1']))\n",
    "                    except NotUniqueError as exc:\n",
    "                        if exc.error_code() == sbol2.SBOLErrorCode.SBOL_ERROR_URI_NOT_UNIQUE:\n",
    "                            return sink_doc.getComponentDefinition('/'.join([sbol2.getHomespace(),\n",
    "                                                                             comp_definition.displayId, '1']))\n",
    "                        else:\n",
    "                            raise\n",
    "\n",
    "                cls.strip_origin_properties(definition_copy, strip_prefixes)\n",
    "                for sub_comp_copy in definition_copy.components:\n",
    "                    cls.strip_origin_properties(sub_comp_copy, strip_prefixes)\n",
    "                for anno_copy in definition_copy.sequenceAnnotations:\n",
    "                    cls.strip_origin_properties(anno_copy, strip_prefixes)\n",
    "\n",
    "                    for loc_copy in anno_copy.locations:\n",
    "                        cls.strip_origin_properties(loc_copy, strip_prefixes)\n",
    "\n",
    "                if make_variant:\n",
    "                    cls.make_variant_definition(sink_doc, definition_copy)\n",
    "            else:\n",
    "                try:\n",
    "                    sink_doc.getComponentDefinition(comp_definition.identity)\n",
    "\n",
    "                    return None\n",
    "                except RuntimeError:\n",
    "                    definition_copy = comp_definition.copy(sink_doc)\n",
    "                except NotFoundError as exc:\n",
    "                    if is_sbol_not_found(exc):\n",
    "                        definition_copy = comp_definition.copy(sink_doc)\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "            if shallow_copy:\n",
    "                definition_copy.sequences = list(comp_definition.sequences)\n",
    "            elif import_sequences:\n",
    "                if len(seqs) > 0:\n",
    "                    seq_copy = cls.copy_sequence(seqs[0], source_doc, sink_doc, True, strip_prefixes)\n",
    "\n",
    "                    if make_variant:\n",
    "                        cls.make_variant_sequence(sink_doc, seq_copy)\n",
    "\n",
    "                    if seq_elements:\n",
    "                        seq_copy.elements = seq_elements\n",
    "\n",
    "                    if parent_doc:\n",
    "                        for parent_definition in parent_definitions:\n",
    "                            if sbol2.BIOPAX_DNA in parent_definition.types:\n",
    "                                parent_seqs = cls.get_DNA_sequences(parent_definition, parent_doc)\n",
    "\n",
    "                                if len(parent_seqs) > 0:\n",
    "                                    seq_copy.wasDerivedFrom = seq_copy.wasDerivedFrom + [parent_seqs[0].identity]\n",
    "                            elif len(parent_definition.sequences) > 0:\n",
    "                                seq_copy.wasDerivedFrom = seq_copy.wasDerivedFrom + [parent_definition.sequences[0].identity]\n",
    "\n",
    "                    definition_copy.sequences = [seq_copy.identity]\n",
    "                else:\n",
    "                    return None\n",
    "            else:\n",
    "                for seq_URI in comp_definition.sequences:\n",
    "                    seq = source_doc.getSequence(seq_URI)\n",
    "\n",
    "                    cls.copy_sequence(seq, source_doc, sink_doc, False, strip_prefixes)\n",
    "\n",
    "                definition_copy.sequences = list(comp_definition.sequences)\n",
    "\n",
    "            if make_variant:\n",
    "                definition_copy.sequenceAnnotations = []\n",
    "                definition_copy.components = []\n",
    "            else:\n",
    "                for seq_anno in comp_definition.sequenceAnnotations:\n",
    "                    if seq_anno.component:\n",
    "                        sub_comp = comp_definition.components.get(seq_anno.component)\n",
    "\n",
    "                        sub_copy = definition_copy.components.get(sub_comp.displayId)\n",
    "\n",
    "                        anno_copy = definition_copy.sequenceAnnotations.get(seq_anno.displayId)\n",
    "                        anno_copy.component = sub_copy.identity\n",
    "\n",
    "                for sub_comp in comp_definition.components:\n",
    "                    try:\n",
    "                        sub_definition = source_doc.getComponentDefinition(sub_comp.definition)\n",
    "                    except RuntimeError:\n",
    "                        sub_definition = None\n",
    "                    except NotFoundError as exc:\n",
    "                        if is_sbol_not_found(exc):\n",
    "                            sub_definition = None\n",
    "                        else:\n",
    "                            raise\n",
    "\n",
    "                    sub_copy = definition_copy.components.get(sub_comp.displayId)\n",
    "\n",
    "                    if sub_definition:\n",
    "                        if shallow_copy:\n",
    "                            sub_copy.definition = sub_definition.identity\n",
    "                        else:\n",
    "                            sub_definition_copy = cls.copy_component_definition(sub_definition, source_doc, sink_doc,\n",
    "                                                                                import_namespace, min_seq_length,\n",
    "                                                                                shallow_copy=shallow_copy,\n",
    "                                                                                strip_prefixes=strip_prefixes)\n",
    "\n",
    "                            if sub_definition_copy:\n",
    "                                sub_copy.definition = sub_definition_copy.identity\n",
    "                            else:\n",
    "                                sub_copy.definition = sub_definition.identity\n",
    "                    else:\n",
    "                        sub_copy.definition = sub_comp.definition\n",
    "\n",
    "            for parent_definition in parent_definitions:\n",
    "                definition_copy.wasDerivedFrom = definition_copy.wasDerivedFrom + [parent_definition.identity]\n",
    "\n",
    "            return definition_copy\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "class FeatureAnnotater():\n",
    "\n",
    "    def __init__(self, feature_library, min_feature_length):\n",
    "        self.feature_library = feature_library\n",
    "        self.feature_matcher = KeywordProcessor()\n",
    "\n",
    "        self.logger = logging.getLogger('synbict')\n",
    "\n",
    "        for feature in feature_library.features:\n",
    "            inline_elements = ' '.join(feature.nucleotides)\n",
    "\n",
    "            if self.__has_min_length(feature, min_feature_length):\n",
    "                if inline_elements in self.feature_matcher:\n",
    "                    if feature.is_non_generic():\n",
    "                        canonical_features = [cf for cf in self.feature_matcher.get_keyword(inline_elements) if\n",
    "                                              cf.is_non_generic()]\n",
    "\n",
    "                        canonical_features.append(feature)\n",
    "                else:\n",
    "                    canonical_features = [feature]\n",
    "\n",
    "                self.feature_matcher.add_keyword(inline_elements, canonical_features)\n",
    "\n",
    "    def get_updated_documents(self):\n",
    "        return self.feature_library.get_updated_documents()\n",
    "\n",
    "    @classmethod\n",
    "    def __has_min_length(cls, feature, min_feature_length):\n",
    "        return min_feature_length == 0 or len(feature.nucleotides) >= min_feature_length\n",
    "\n",
    "    @classmethod\n",
    "    def __create_sub_component(cls, parent_definition, child_definition):\n",
    "        i = 1\n",
    "\n",
    "        while i > 0:\n",
    "            try:\n",
    "                sub_comp = parent_definition.components.create('_'.join([child_definition.displayId,\n",
    "                                                                         'comp',\n",
    "                                                                         str(i)]))\n",
    "            except RuntimeError:\n",
    "                sub_comp = None\n",
    "            except NotUniqueError as exc:\n",
    "                if exc.error_code() == sbol2.SBOLErrorCode.SBOL_ERROR_URI_NOT_UNIQUE:\n",
    "                    sub_comp = None\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "            if sub_comp is None:\n",
    "                i = i + 1\n",
    "            else:\n",
    "                sub_comp.name = child_definition.name\n",
    "                sub_comp.definition = child_definition.identity\n",
    "\n",
    "                sub_comp.roleIntegration = None\n",
    "\n",
    "                i = -1\n",
    "\n",
    "        return sub_comp\n",
    "\n",
    "    @classmethod\n",
    "    def __create_sequence_annotation(cls, parent_definition, child_definition, orientation, start, end,\n",
    "                                     sub_comp_URI=None, parent_URI=None):\n",
    "        i = 1\n",
    "\n",
    "        while i > 0:\n",
    "            try:\n",
    "                seq_anno = parent_definition.sequenceAnnotations.create('_'.join([child_definition.displayId,\n",
    "                                                                                  'anno',\n",
    "                                                                                  str(i)]))\n",
    "            except RuntimeError:\n",
    "                seq_anno = None\n",
    "            except NotUniqueError as exc:\n",
    "                if exc.error_code() == sbol2.SBOLErrorCode.SBOL_ERROR_URI_NOT_UNIQUE:\n",
    "                    seq_anno = None\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "            if seq_anno is None:\n",
    "                i = i + 1\n",
    "            else:\n",
    "                seq_anno.name = child_definition.name\n",
    "                seq_anno.description = child_definition.description\n",
    "                if sub_comp_URI:\n",
    "                    seq_anno.component = sub_comp_URI\n",
    "                if parent_URI:\n",
    "                    seq_anno.roles = seq_anno.roles + child_definition.roles\n",
    "                    seq_anno.wasDerivedFrom = seq_anno.wasDerivedFrom + [parent_URI]\n",
    "                \n",
    "                location = seq_anno.locations.createRange('_'.join([seq_anno.displayId,\n",
    "                                                                    'loc']))\n",
    "\n",
    "                location.orientation = orientation\n",
    "                location.start = start\n",
    "                location.end = end\n",
    "\n",
    "                i = -1\n",
    "\n",
    "        return seq_anno\n",
    "\n",
    "    def __process_feature_matches(self, target_doc, target_definition, feature_matches, orientation, target_length,\n",
    "                                  rc_factor=0, copy_definitions=True, complete_matches=False):\n",
    "        for feature_match in feature_matches:\n",
    "            temp_start = feature_match[1]//2 + 1\n",
    "            temp_end = (feature_match[2] + 1)//2\n",
    "\n",
    "            if rc_factor > 0:\n",
    "                start = rc_factor - temp_end\n",
    "                end = rc_factor - temp_start\n",
    "            else:\n",
    "                start = temp_start\n",
    "                end = temp_end\n",
    "\n",
    "            for feature in feature_match[0]:\n",
    "                if len(feature.nucleotides) < target_length or complete_matches:\n",
    "                    feature_definition = self.feature_library.get_definition(feature.identity)\n",
    "\n",
    "                    if feature_definition.name is None:\n",
    "                        feature_ID = feature_definition.displayId\n",
    "                    else:\n",
    "                        feature_ID = feature_definition.name\n",
    "\n",
    "                    feature_role = FeaturePruner.get_common_role(feature_definition.roles)\n",
    "\n",
    "                    sub_comp = self.__create_sub_component(target_definition, feature_definition)\n",
    "                    self.__create_sequence_annotation(target_definition, feature_definition, orientation, start, end,\n",
    "                                                      sub_comp.identity)\n",
    "\n",
    "                    if copy_definitions:\n",
    "                        feature_doc = self.feature_library.get_document(feature.identity)\n",
    "\n",
    "                        FeatureLibrary.copy_component_definition(feature_definition, feature_doc, target_doc)\n",
    "\n",
    "                    self.logger.debug('Annotated %s (%s, %s) at [%s, %s] in %s', feature_definition.identity, feature_ID,\n",
    "                                      feature_role, start, end, target_definition.identity)\n",
    "\n",
    "    def extend_features_by_name(self, target_library, min_target_length, mismatch_threshold, strip_prefixes=[]):\n",
    "        self.logger.info('Extending feature library')\n",
    "\n",
    "        aligner = Align.PairwiseAligner()\n",
    "        aligner.match_score = 1\n",
    "        aligner.mismatch_score = -2\n",
    "        aligner.internal_gap_score = -2.5\n",
    "\n",
    "        for target in target_library.features:\n",
    "            if self.__has_min_length(target, min_target_length):\n",
    "                target_doc = target_library.get_document(target.identity)\n",
    "\n",
    "                target_definition = target_doc.getComponentDefinition(target.identity)\n",
    "\n",
    "                for seq_anno in target_definition.sequenceAnnotations:\n",
    "                    if (seq_anno.name and not seq_anno.component and len(seq_anno.locations) == 1\n",
    "                            and seq_anno.locations[0].getTypeURI() == sbol2.SBOL_RANGE):\n",
    "                        anno_start = seq_anno.locations.getRange().start\n",
    "                        anno_end = seq_anno.locations.getRange().end\n",
    "\n",
    "                        target_nucleotides = target.nucleotides[anno_start - 1:anno_end].upper()\n",
    "                        rc_target_nucleotides = str(Seq(target_nucleotides).reverse_complement()).upper()\n",
    "\n",
    "                        inline_elements = ' '.join(target_nucleotides)\n",
    "                        rc_elements = ' '.join(rc_target_nucleotides)\n",
    "\n",
    "                        if not inline_elements in self.feature_matcher and not rc_elements in self.feature_matcher:\n",
    "                            feature_definitions = self.feature_library.get_definitions_by_name(seq_anno.name)\n",
    "\n",
    "                            for feature_definition in feature_definitions:\n",
    "                                if set(seq_anno.roles) == set(feature_definition.roles):\n",
    "                                    feature_doc = self.feature_library.get_document(feature_definition.identity)\n",
    "\n",
    "                                    feature_seqs = FeatureLibrary.get_DNA_sequences(feature_definition, feature_doc)\n",
    "                                    \n",
    "                                    if len(feature_seqs) > 0:\n",
    "                                        feature_nucleotides = feature_seqs[0].elements.upper()\n",
    "\n",
    "                                        score = aligner.score(target_nucleotides, feature_nucleotides)\n",
    "                                        rc_score = aligner.score(rc_target_nucleotides, feature_nucleotides)\n",
    "\n",
    "                                        self.logger.debug('%s score %s', seq_anno.name, str(score))\n",
    "                                        self.logger.debug('%s rc score %s', seq_anno.name, str(rc_score))\n",
    "                                        self.logger.debug('target nucleotides: %s', target_nucleotides)\n",
    "                                        self.logger.debug('feature nucleotides: %s', feature_nucleotides)\n",
    "\n",
    "                                        if rc_score > score:\n",
    "                                            best_score = rc_score\n",
    "                                            best_nucleotides = rc_target_nucleotides\n",
    "                                        else:\n",
    "                                            best_score = score\n",
    "                                            best_nucleotides = target_nucleotides\n",
    "\n",
    "                                        if len(target_nucleotides) < len(feature_nucleotides):\n",
    "                                            max_score = len(target_nucleotides)\n",
    "                                        else:\n",
    "                                            max_score = len(feature_nucleotides)\n",
    "\n",
    "                                        if max_score - best_score < mismatch_threshold*max_score:\n",
    "                                            variant_definition = FeatureLibrary.copy_component_definition(feature_definition,\n",
    "                                                feature_doc, feature_doc, import_namespace=True, import_sequences=True,\n",
    "                                                seq_elements=target_nucleotides, parent_definitions=[target_definition],\n",
    "                                                parent_doc=target_doc, make_variant=True, strip_prefixes=strip_prefixes)\n",
    "\n",
    "                                            if variant_definition:\n",
    "                                                sub_identities = []\n",
    "                                                for sub_comp in variant_definition.components:\n",
    "                                                    sub_identities.append(sub_comp.definition)\n",
    "\n",
    "                                                feature = Feature(feature_nucleotides,\n",
    "                                                                  variant_definition.identity,\n",
    "                                                                  variant_definition.roles,\n",
    "                                                                  sub_identities,\n",
    "                                                                  variant_definition.wasDerivedFrom)\n",
    "\n",
    "                                                self.feature_matcher.add_keyword(inline_elements, [feature])\n",
    "\n",
    "                                                self.logger.debug('Extended feature library with %s', variant_definition.identity)\n",
    "\n",
    "        self.feature_library.update()\n",
    "\n",
    "        self.logger.info('Finished extending feature library')\n",
    "\n",
    "    def annotate_raw_sequences(self, raw_seqs, comp_IDs=[], min_target_length=0, complete_matches=False,\n",
    "                               strip_prefixes=[]):\n",
    "        annotated_comps = []\n",
    "\n",
    "        if not isinstance(raw_seqs, list):\n",
    "            raw_seqs = [raw_seqs]\n",
    "\n",
    "        if not isinstance(comp_IDs, list):\n",
    "            comp_IDs = [comp_IDs]\n",
    "\n",
    "        for i in range(0, len(raw_seqs)):\n",
    "            target_doc = sbol2.Document()\n",
    "\n",
    "            if i < len(comp_IDs):\n",
    "                comp_ID = comp_IDs[i]\n",
    "            else:\n",
    "                comp_ID = 'construct_' + str(i + 1)\n",
    "\n",
    "            target_comp = sbol2.ComponentDefinition(comp_ID, sbol2.BIOPAX_DNA, '1')\n",
    "            target_comp.sequence = sbol2.Sequence(comp_ID + '_seq', raw_seqs[i], sbol2.SBOL_ENCODING_IUPAC, '1')\n",
    "\n",
    "            annotated_comps.append(target_comp)\n",
    "\n",
    "            target_doc.addComponentDefinition(target_comp)\n",
    "\n",
    "            target_library = FeatureLibrary([target_doc])\n",
    "\n",
    "            self.annotate(target_library, min_target_length, True, complete_matches=complete_matches,\n",
    "                          strip_prefixes=strip_prefixes)\n",
    "\n",
    "        if len(annotated_comps) == 1:\n",
    "            return annotated_comps[0]\n",
    "        else:\n",
    "            return annotated_comps\n",
    "\n",
    "    def annotate(self, target_library, min_target_length, in_place=False, output_library=None, complete_matches=False,\n",
    "                 strip_prefixes=[]):\n",
    "        annotated_identities = []\n",
    "\n",
    "        for target in target_library.features:\n",
    "            if self.__has_min_length(target, min_target_length):\n",
    "                self.logger.info('Annotating %s', target.identity)\n",
    "\n",
    "                inline_elements = ' '.join(target.nucleotides)\n",
    "                rc_elements = ' '.join(target.reverse_complement_nucleotides())\n",
    "\n",
    "                inline_matches = self.feature_matcher.extract_keywords(inline_elements, span_info=True)\n",
    "                rc_matches = self.feature_matcher.extract_keywords(rc_elements, span_info=True)\n",
    "\n",
    "                if len(inline_matches) > 0 or len(rc_matches) > 0:\n",
    "                    target_doc = target_library.get_document(target.identity)\n",
    "\n",
    "                    target_definition = target_doc.getComponentDefinition(target.identity)\n",
    "\n",
    "                    doc_index = target_library.get_document_index(target.identity)\n",
    "                    \n",
    "                    if output_library and doc_index < len(output_library.docs):\n",
    "                        output_doc = output_library.docs[doc_index]\n",
    "\n",
    "                        if in_place:\n",
    "                            definition_copy = FeatureLibrary.copy_component_definition(target_definition,\n",
    "                                                                                       target_doc,\n",
    "                                                                                       output_doc,\n",
    "                                                                                       min_seq_length=min_target_length,\n",
    "                                                                                       shallow_copy=True,\n",
    "                                                                                       strip_prefixes=strip_prefixes)\n",
    "                        else:\n",
    "                            definition_copy = FeatureLibrary.copy_component_definition(target_definition,\n",
    "                                                                                       target_doc,\n",
    "                                                                                       output_doc, True,\n",
    "                                                                                       min_target_length,\n",
    "                                                                                       shallow_copy=True,\n",
    "                                                                                       strip_prefixes=strip_prefixes)\n",
    "                    elif in_place:\n",
    "                        definition_copy = target_definition\n",
    "                    else:\n",
    "                        definition_copy = FeatureLibrary.copy_component_definition(target_definition, target_doc,\n",
    "                                                                                   target_doc, True,\n",
    "                                                                                   min_target_length,\n",
    "                                                                                   strip_prefixes=strip_prefixes)\n",
    "\n",
    "                    if definition_copy:\n",
    "                        self.__process_feature_matches(target_doc, definition_copy, inline_matches,\n",
    "                            sbol2.SBOL_ORIENTATION_INLINE, len(target.nucleotides),\n",
    "                            copy_definitions=(not output_library or doc_index >= len(output_library.docs)),\n",
    "                            complete_matches=complete_matches)\n",
    "                        self.__process_feature_matches(target_doc, definition_copy, rc_matches,\n",
    "                            sbol2.SBOL_ORIENTATION_REVERSE_COMPLEMENT, len(target.nucleotides), len(target.nucleotides) + 1,\n",
    "                            (not output_library or doc_index >= len(output_library.docs)),\n",
    "                            complete_matches=complete_matches)\n",
    "\n",
    "                        annotated_identities.append(definition_copy.identity)\n",
    "                    else:\n",
    "                        self.logger.warning('%s was not annotated because its version could not be incremented.',\n",
    "                                        target.identity)\n",
    "\n",
    "                self.logger.info('Finished annotating %s', target.identity)\n",
    "\n",
    "        return annotated_identities\n",
    "\n",
    "class FeaturePruner():\n",
    "\n",
    "    COMMON_ROLE_DICT = {\n",
    "        sbol2.SO_PROMOTER: 'promoter',\n",
    "        sbol2.SO_CDS: 'CDS',\n",
    "        sbol2.SO_TERMINATOR: 'terminator',\n",
    "        'http://identifiers.org/so/SO:0001977': 'ribonuclease_site'\n",
    "    }\n",
    "\n",
    "    def __init__(self, feature_library, roles=set()):\n",
    "        self.feature_library = feature_library\n",
    "        self.roles = roles\n",
    "\n",
    "        self.logger = logging.getLogger('synbict')\n",
    "\n",
    "    @classmethod\n",
    "    def __has_min_length(cls, feature, min_feature_length):\n",
    "        return min_feature_length == 0 or len(feature.nucleotides) >= min_feature_length\n",
    "\n",
    "    @classmethod\n",
    "    def __is_covered(cls, anno, cover_annos, cover_offset):\n",
    "        for cover_anno in cover_annos:\n",
    "            if not abs(cover_anno[0] - anno[0]) <= cover_offset or not abs(cover_anno[1] - anno[1]) <= cover_offset:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def __remove_annotations(self, indices, annos, target_definition):\n",
    "        for i in range(len(annos) - 1, -1, -1):\n",
    "            if annos[i][5] is None:\n",
    "                feature_identity = annos[i][2]\n",
    "            else:\n",
    "                feature_identity = target_definition.components.get(annos[i][5]).definition\n",
    "            \n",
    "            if i in indices:\n",
    "                target_definition.sequenceAnnotations.remove(annos[i][2])\n",
    "\n",
    "                if annos[i][5]:\n",
    "                    target_definition.components.remove(annos[i][5])\n",
    "\n",
    "                self.logger.debug('Removed %s at [%s, %s] in %s', feature_identity, annos[i][0], annos[i][1],\n",
    "                              target_definition.identity)\n",
    "\n",
    "                del annos[i]\n",
    "\n",
    "    @classmethod\n",
    "    def get_common_role(cls, roles):\n",
    "        for role in roles:\n",
    "            if role in cls.COMMON_ROLE_DICT:\n",
    "                return cls.COMMON_ROLE_DICT[role]\n",
    "\n",
    "        return 'sequence_feature'\n",
    "\n",
    "    @classmethod\n",
    "    def __ask_swap_annotations(cls, anno, sub_part_anno, target_definition):\n",
    "        if anno[4] is None:\n",
    "            anno_ID = anno[3]\n",
    "        else:\n",
    "            anno_ID = anno[4]\n",
    "\n",
    "        if sub_part_anno[4] is None:\n",
    "            sub_part_anno_ID = sub_part_anno[3]\n",
    "        else:\n",
    "            sub_part_anno_ID = sub_part_anno[4]\n",
    "\n",
    "        select_message = 'Annotation {an} ({ai}) and\\nsub-part annotation {sp} ({si})\\nat [{st}, {en}] in {td} appear \\\n",
    "to be nearly identical.\\nRemove second annotation and link first to sub-part? \\\n",
    "(0=no,1=yes):\\n'.format(an=anno[2],\n",
    "                        ai=anno_ID,\n",
    "                        sp=sub_part_anno[2],\n",
    "                        si=sub_part_anno_ID,\n",
    "                        st=anno[0],\n",
    "                        en=anno[1],\n",
    "                        td=target_definition.identity)\n",
    "\n",
    "        selected_message = input(select_message)\n",
    "\n",
    "        try:\n",
    "            return int(selected_message.strip()) == 1\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    @classmethod\n",
    "    def __select_annotations(cls, doc, target_definition, annos, ask_user=True, canonical_library=None, keep_flat=True):\n",
    "        kept_indices = []\n",
    "\n",
    "        feature_messages = []\n",
    "\n",
    "        for i in range(0, len(annos)):\n",
    "            if annos[i][5] is None:\n",
    "                if annos[i][4] is None:\n",
    "                    feature_ID = annos[i][3]\n",
    "                else:\n",
    "                    feature_ID = annos[i][4]\n",
    "\n",
    "                feature_role = cls.get_common_role(annos[i][6])\n",
    "\n",
    "                if ask_user:\n",
    "                    if annos[i][7] and len(annos[i][7]) > 0:\n",
    "                        if len(feature_role) > 0:\n",
    "                            feature_messages.append('{nx}: {id} ({fi}, {ro}) at [{st}, {en}]. {de}'.format(nx=str(i), id=annos[i][2],\n",
    "                                fi=feature_ID, ro=feature_role, st=annos[i][0], en=annos[i][1], de=annos[i][7]))\n",
    "                        else:\n",
    "                            feature_messages.append('{nx}: {id} ({fi}) at [{st}, {en}]. {de}'.format(nx=str(i), id=annos[i][2],\n",
    "                                fi=feature_ID, st=annos[i][0], en=annos[i][1], de=annos[i][7]))\n",
    "                    elif len(feature_role) > 0:\n",
    "                        feature_messages.append('{nx}: {id} ({fi}, {ro}) at [{st}, {en}]'.format(nx=str(i), id=annos[i][2],\n",
    "                            fi=feature_ID, ro=feature_role, st=annos[i][0], en=annos[i][1]))\n",
    "                    else:\n",
    "                        feature_messages.append('{nx}: {id} ({fi}) at [{st}, {en}]'.format(nx=str(i), id=annos[i][2],\n",
    "                            fi=feature_ID, st=annos[i][0], en=annos[i][1]))\n",
    "                elif keep_flat:\n",
    "                    kept_indices.append(i)\n",
    "            else:\n",
    "                feature_identity = target_definition.components.get(annos[i][5]).definition\n",
    "\n",
    "                if ask_user:\n",
    "                    feature_definition = doc.getComponentDefinition(feature_identity)\n",
    "                    \n",
    "                    if feature_definition.name is None:\n",
    "                        feature_ID = feature_definition.displayId\n",
    "                    else:\n",
    "                        feature_ID = feature_definition.name\n",
    "\n",
    "                    feature_role = cls.get_common_role(feature_definition.roles)\n",
    "\n",
    "                    feature_description = feature_definition.description\n",
    "\n",
    "                    if feature_description and len(feature_description) > 0:\n",
    "                        if len(feature_role) > 0:\n",
    "                            feature_messages.append('{nx}: {id} ({fi}, {ro}) at [{st}, {en}]. {de}'.format(nx=str(i), id=feature_identity,\n",
    "                                fi=feature_ID, ro=feature_role, st=annos[i][0], en=annos[i][1], de=feature_description))\n",
    "                        else:\n",
    "                            feature_messages.append('{nx}: {id} ({fi}) at [{st}, {en}]. {de}'.format(nx=str(i), id=feature_identity,\n",
    "                                fi=feature_ID, st=annos[i][0], en=annos[i][1], de=feature_description))\n",
    "                    elif len(feature_role) > 0:\n",
    "                        feature_messages.append('{nx}: {id} ({fi}, {ro}) at [{st}, {en}]'.format(nx=str(i), id=feature_identity,\n",
    "                            fi=feature_ID, ro=feature_role, st=annos[i][0], en=annos[i][1]))\n",
    "                    else:\n",
    "                        feature_messages.append('{nx}: {id} ({fi}) at [{st}, {en}]'.format(nx=str(i), id=feature_identity,\n",
    "                            fi=feature_ID, st=annos[i][0], en=annos[i][1]))\n",
    "                elif not canonical_library or canonical_library.has_feature(feature_identity):\n",
    "                    kept_indices.append(i)\n",
    "\n",
    "        if ask_user:\n",
    "            if target_definition.name is None:\n",
    "                target_ID = target_definition.displayId\n",
    "            else:\n",
    "                target_ID = target_definition.name\n",
    "\n",
    "            select_message = 'There appear to be redundant features in {pi}:\\n{fm}\\nPlease select which ones to \\\n",
    "remove if any (comma-separated list of indices, for example 0,2,5):\\n'.format(fm='\\n'.join(feature_messages),\n",
    "                                                                              pi=target_ID)\n",
    "\n",
    "            selected_message = input(select_message)\n",
    "\n",
    "            try:\n",
    "                selected_indices = [int(si.strip()) for si in selected_message.split(',')]\n",
    "            except ValueError:\n",
    "                selected_indices = []\n",
    "\n",
    "            return set(selected_indices)\n",
    "        else:\n",
    "            return set(range(0, len(annos))).difference(set(kept_indices))\n",
    "\n",
    "    def __filter_annotations(self, annos, target_definition):\n",
    "        for i in range(len(annos) - 1, -1, -1):\n",
    "            if annos[i][5] is None:\n",
    "                feature_identity = annos[i][2]\n",
    "\n",
    "                feature_roles = annos[i][6]\n",
    "            else:\n",
    "                feature_identity = target_definition.components.get(annos[i][5]).definition\n",
    "\n",
    "                feature_roles = set(self.feature_library.get_definition(feature_identity).roles)\n",
    "\n",
    "            if len(feature_roles.intersection(self.roles)) == 0:\n",
    "                target_definition.sequenceAnnotations.remove(annos[i][2])\n",
    "\n",
    "                if annos[i][5]:\n",
    "                    target_definition.components.remove(annos[i][5])\n",
    "\n",
    "                self.logger.debug('Removed %s at [%s, %s] in %s', feature_identity, annos[i][0], annos[i][1],\n",
    "                              target_definition.identity)\n",
    "\n",
    "                del annos[i]\n",
    "\n",
    "    def __are_swappable(self, anno, sub_part_anno, target_definition):\n",
    "        if not anno[5] and sub_part_anno[5] and anno[0] == sub_part_anno[0] and anno[1] == sub_part_anno[1]:\n",
    "            feature_identity = target_definition.components.get(sub_part_anno[5]).definition\n",
    "\n",
    "            feature_definition = self.feature_library.get_definition(feature_identity)\n",
    "\n",
    "            return (not Feature.has_non_generic_role(anno[6]) or\n",
    "                    anno[6] == set(feature_definition.roles))\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __swap_annotations(self, anno, sub_part_anno, target_definition):\n",
    "        seq_anno = target_definition.sequenceAnnotations.get(anno[2])\n",
    "\n",
    "        seq_anno.roles = []\n",
    "        seq_anno.component = sub_part_anno[5]\n",
    "\n",
    "        target_definition.sequenceAnnotations.remove(sub_part_anno[2])\n",
    "\n",
    "        self.logger.debug('Removed %s at [%s, %s] in %s', sub_part_anno[2], sub_part_anno[0], sub_part_anno[1],\n",
    "                      target_definition.identity)\n",
    "        self.logger.debug('Modified %s at [%s, %s] in %s to refer to %s', anno[2], anno[0], anno[1],\n",
    "                      target_definition.identity, sub_part_anno[5])\n",
    "\n",
    "    # @classmethod\n",
    "    # def __get_annotations(cls, doc, comp_definition):\n",
    "    #     cut_annos = [(sa.locations.getCut().at, sa.locations.getCut().at, sa.identity, sa.displayId, sa.name, sa.component, set(sa.roles)) for sa in comp_definition.sequenceAnnotations if len(sa.locations) == 1 and sa.locations[0].getTypeURI() == SBOL_CUT]\n",
    "    #     annos = [(sa.locations.getRange().start, sa.locations.getRange().end, sa.identity, sa.displayId, sa.name, sa.component, set(sa.roles)) for sa in comp_definition.sequenceAnnotations if len(sa.locations) == 1 and sa.locations[0].getTypeURI() == SBOL_RANGE]\n",
    "            \n",
    "    #     annos.extend(cut_annos)\n",
    "\n",
    "    #     for sub_comp in comp_definition.components:\n",
    "    #         try:\n",
    "    #             sub_definition = doc.getComponentDefinition(comp_definition.identity)\n",
    "    #         except RuntimeError:\n",
    "    #             sub_definition = None\n",
    "\n",
    "    #         if sub_definition is not None:\n",
    "    #             annos.extend(cls.__get_annotations(doc, sub_definition))\n",
    "\n",
    "    #     return annos\n",
    "\n",
    "    @classmethod\n",
    "    def __get_flat_annotation_indices(cls, anno_group):\n",
    "        flat_indices = []\n",
    "\n",
    "        for i in range(0, len(anno_group)):\n",
    "            if anno_group[i][5] is None:\n",
    "                flat_indices.append(i)\n",
    "\n",
    "        return flat_indices\n",
    "\n",
    "    def clean(self, feature_library, annotated_features, annotating_features):\n",
    "        self.logger.info('Cleaning up')\n",
    "\n",
    "        sub_definitions = set()\n",
    "\n",
    "        for annotated_feature in annotated_features:\n",
    "            annotated_doc = feature_library.get_document(annotated_feature.identity)\n",
    "            annotated_definition = annotated_doc.getComponentDefinition(annotated_feature.identity)\n",
    "\n",
    "            sub_IDs = set()\n",
    "            temp_sub_definitions = set()\n",
    "\n",
    "            for comp in annotated_definition.components:\n",
    "                sub_IDs.add(comp.displayId)\n",
    "                temp_sub_definitions.add(comp.definition)\n",
    "            for seq_anno in annotated_definition.sequenceAnnotations:\n",
    "                sub_IDs.add(seq_anno.displayId)\n",
    "\n",
    "            parent_sub_IDs = set()\n",
    "\n",
    "            for parent_identity in annotated_definition.wasDerivedFrom:\n",
    "                parent_doc = feature_library.get_document(parent_identity)\n",
    "\n",
    "                if parent_doc:\n",
    "                    parent_definition = parent_doc.getComponentDefinition(parent_identity)\n",
    "\n",
    "                    for comp in parent_definition.components:\n",
    "                        parent_sub_IDs.add(comp.displayId)\n",
    "                    for seq_anno in parent_definition.sequenceAnnotations:\n",
    "                        parent_sub_IDs.add(seq_anno.displayId)\n",
    "\n",
    "            if len(sub_IDs) == len(parent_sub_IDs) and len(sub_IDs) == len(sub_IDs.intersection(parent_sub_IDs)):\n",
    "                annotated_doc.componentDefinitions.remove(annotated_feature.identity)\n",
    "\n",
    "                self.logger.debug('Removed %s from %s', annotated_feature.identity, annotated_doc.name)\n",
    "            else:\n",
    "                sub_definitions.update(temp_sub_definitions)\n",
    "\n",
    "        for annotating_feature in annotating_features:\n",
    "            if annotating_feature.identity not in sub_definitions:\n",
    "                annotating_doc = feature_library.get_document(annotating_feature.identity)\n",
    "                annotating_definition = annotating_doc.getComponentDefinition(annotating_feature.identity)\n",
    "\n",
    "                annotating_doc.componentDefinitions.remove(annotating_feature.identity)\n",
    "                for seq_identity in annotating_definition.sequences:\n",
    "                    try:\n",
    "                        annotating_doc.sequences.remove(seq_identity)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "                self.logger.debug('Removed %s from %s', annotating_feature.identity, annotating_doc.name)\n",
    "\n",
    "        self.logger.info('Finished cleaning up')\n",
    "\n",
    "    def prune(self, target_library, cover_offset, min_target_length, ask_user=True, canonical_library=None,\n",
    "              delete_flat=False, keep_flat=True, target_features=[], auto_swap=False, require_sequence=True):\n",
    "        target_identities = set()\n",
    "        for target_feature in target_features:\n",
    "            target_identities.add(target_feature.identity)\n",
    "\n",
    "        for target in target_library.features:\n",
    "            if ((not require_sequence or self.__has_min_length(target, min_target_length))\n",
    "                    and (len(target_identities) == 0 or target.identity in target_identities)):\n",
    "                self.logger.info('Pruning %s', target.identity)\n",
    "\n",
    "                target_doc = target_library.get_document(target.identity)\n",
    "\n",
    "                target_definition = target_doc.getComponentDefinition(target.identity)\n",
    "\n",
    "                cut_annos = [(sa.locations.getCut().at, sa.locations.getCut().at, sa.identity, sa.displayId, sa.name, sa.component, set(sa.roles), sa.description) for sa in target_definition.sequenceAnnotations if len(sa.locations) == 1 and sa.locations[0].getTypeURI() == sbol2.SBOL_CUT]\n",
    "                annos = [(sa.locations.getRange().start, sa.locations.getRange().end, sa.identity, sa.displayId, sa.name, sa.component, set(sa.roles), sa.description) for sa in target_definition.sequenceAnnotations if len(sa.locations) == 1 and sa.locations[0].getTypeURI() == sbol2.SBOL_RANGE]\n",
    "                \n",
    "                annos.extend(cut_annos)\n",
    "\n",
    "                if len(self.roles) > 0:\n",
    "                    self.__filter_annotations(annos, target_definition)\n",
    "\n",
    "                annos.sort()\n",
    "\n",
    "                grouped_annos = [[]]\n",
    "\n",
    "                for anno in annos:\n",
    "                    if len(grouped_annos) > 1 and self.__is_covered(anno, grouped_annos[-2], cover_offset):\n",
    "                        grouped_annos[-2].append(anno)\n",
    "                    elif self.__is_covered(anno, grouped_annos[-1], cover_offset):\n",
    "                        grouped_annos[-1].append(anno)\n",
    "                    else:\n",
    "                        grouped_annos.append([anno])\n",
    "\n",
    "                if delete_flat:\n",
    "                    for anno_group in grouped_annos:\n",
    "                        flat_indices = self.__get_flat_annotation_indices(anno_group)\n",
    "\n",
    "                        self.__remove_annotations(flat_indices, anno_group, target_definition)\n",
    "\n",
    "                for anno_group in grouped_annos:\n",
    "                    if len(anno_group) > 1:\n",
    "                        selected_indices = self.__select_annotations(target_doc, target_definition, anno_group,\n",
    "                            ask_user, canonical_library, keep_flat)\n",
    "\n",
    "                        self.__remove_annotations(selected_indices, anno_group, target_definition)\n",
    "\n",
    "                for anno_group in grouped_annos:\n",
    "                    if ask_user or auto_swap:\n",
    "                        if len(anno_group) == 2:\n",
    "                            if (self.__are_swappable(anno_group[0], anno_group[1], target_definition) and\n",
    "                                    (auto_swap or\n",
    "                                    self.__ask_swap_annotations(anno_group[0], anno_group[1], target_definition))):\n",
    "                                self.__swap_annotations(anno_group[0], anno_group[1], target_definition)\n",
    "                            elif (self.__are_swappable(anno_group[1], anno_group[0], target_definition) and\n",
    "                                    (auto_swap or\n",
    "                                    self.__ask_swap_annotations(anno_group[1], anno_group[0], target_definition))):\n",
    "                                self.__swap_annotations(anno_group[1], anno_group[0], target_definition)\n",
    "\n",
    "                for anno_group in grouped_annos:\n",
    "                    if len(anno_group) > 1:\n",
    "                        redundant_URIs = []\n",
    "\n",
    "                        for anno in anno_group:\n",
    "                            if anno[5]:\n",
    "                                redundant_URIs.append(target_definition.components.get(anno[5]).definition)\n",
    "                            else:\n",
    "                                redundant_URIs.append(anno[2])\n",
    "\n",
    "                        self.logger.debug('Detected potentially redundant sub-parts in %s: %s.',\n",
    "                                          target_definition.identity, \n",
    "                                          str(redundant_URIs))\n",
    "\n",
    "                self.logger.info('Finished pruning %s', target.identity)\n",
    "\n",
    "def curate(feature_library, target_library, output_library, output_files, extend_features, no_annotation,\n",
    "           min_feature_length, min_target_length, extension_threshold, extension_suffix, in_place, minimal_output,\n",
    "           no_pruning, deletion_roles, cover_offset, delete_flat, auto_swap, non_interactive, logger,\n",
    "           complete_matches=False, strip_prefixes=[]):\n",
    "    if extend_features or not no_annotation:\n",
    "        feature_annotater = FeatureAnnotater(feature_library, min_feature_length)\n",
    "    \n",
    "    feature_curator = FeatureCurator(target_library, output_library)\n",
    "\n",
    "    if extend_features:\n",
    "        feature_curator.extend_features(feature_annotater,\n",
    "                                        min_target_length,\n",
    "                                        extension_threshold,\n",
    "                                        strip_prefixes)\n",
    "\n",
    "        for extended_doc in feature_annotater.get_updated_documents():\n",
    "            (extended_file_base, extended_file_extension) = os.path.splitext(extended_doc.name)\n",
    "\n",
    "            if len(extension_suffix) > 0:\n",
    "                extended_file = '_'.join([extended_file_base, extension_suffix]) + '.xml'\n",
    "            else:\n",
    "                extended_file = extended_file_base + '.xml'\n",
    "\n",
    "            logger.info('Writing %s', extended_file)\n",
    "\n",
    "            extended_doc.write(extended_file)\n",
    "\n",
    "    if no_annotation:\n",
    "        annotated_features = []\n",
    "        annotating_features = []\n",
    "    else:\n",
    "        (annotated_features, annotating_features) = feature_curator.annotate_features(feature_annotater,\n",
    "                                                                                      min_target_length,\n",
    "                                                                                      in_place,\n",
    "                                                                                      complete_matches,\n",
    "                                                                                      strip_prefixes)\n",
    "\n",
    "        if minimal_output:\n",
    "            for i in range(0, len(output_library.docs)):\n",
    "                if len(output_library.docs[i].componentDefinitions) == 0:\n",
    "                    logger.warning('Failed to annotate %s, possibly no constructs found with minimum length %s',\n",
    "                                   target_library.docs[i].name, min_target_length)\n",
    "        else:\n",
    "            for i in target_library.get_non_updated_indices():\n",
    "                logger.warning('Failed to annotate %s, possibly no constructs found with minimum length %s',\n",
    "                               target_library.docs[i].name, min_target_length)\n",
    "\n",
    "    if not no_pruning:\n",
    "        feature_pruner = FeaturePruner(feature_library, set(deletion_roles))\n",
    "\n",
    "        feature_curator.prune_features(feature_pruner,\n",
    "                                       cover_offset,\n",
    "                                       min_target_length,\n",
    "                                       annotated_features,\n",
    "                                       annotating_features,\n",
    "                                       delete_flat,\n",
    "                                       auto_swap,\n",
    "                                       not non_interactive)\n",
    "\n",
    "    if not no_annotation or not no_pruning:\n",
    "        if len(output_library.docs) > 0:\n",
    "            for i in range(0, len(output_library.docs)):\n",
    "                if sbol2.Config.getOption('validate') == True:\n",
    "                    logger.info('Validating and writing %s', output_files[i])\n",
    "                else:\n",
    "                    logger.info('Writing %s', output_files[i])\n",
    "\n",
    "                output_library.docs[i].write(output_files[i])\n",
    "        else:\n",
    "            for i in range(0, len(target_library.docs)):\n",
    "                if sbol2.Config.getOption('validate') == True:\n",
    "                    logger.info('Validating and writing %s', output_files[i])\n",
    "                else:\n",
    "                    logger.info('Writing %s', output_files[i])\n",
    "\n",
    "                target_library.docs[i].write(output_files[i])\n",
    "\n",
    "def download_sequences(doc, synbiohub):\n",
    "    for comp_definition in doc.componentDefinitions:\n",
    "        for seq_URI in comp_definition.sequences:\n",
    "            download_sequence = False\n",
    "\n",
    "            try:\n",
    "                doc.getSequence(seq_URI)\n",
    "            except RuntimeError:\n",
    "                download_sequence = True\n",
    "            except NotFoundError as exc:\n",
    "                if is_sbol_not_found(exc):\n",
    "                    download_sequence = True\n",
    "\n",
    "            if download_sequence:\n",
    "                try:\n",
    "                    synbiohub.pull(seq_URI, doc)\n",
    "                except NotFoundError as exc:\n",
    "                    if is_sbol_not_found(exc):\n",
    "                        logger.warning('Unable to download sequence %s, seq_URI)')\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "def main(args=None):\n",
    "    if args is None:\n",
    "        args = sys.argv[1:]\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Common arguments\n",
    "    parser.add_argument('-n', '--namespace')\n",
    "    parser.add_argument('-t', '--target_files', nargs='*', default=[])\n",
    "    parser.add_argument('-o', '--output_files', nargs='*', default=[])\n",
    "    parser.add_argument('-s', '--output_suffix', nargs='?', default='')\n",
    "    parser.add_argument('-p', '--in_place', action='store_true')\n",
    "    parser.add_argument('-m', '--min_target_length', nargs='?', default=2000)\n",
    "    parser.add_argument('-mo', '--minimal_output', action='store_true')\n",
    "    parser.add_argument('-ni', '--non_interactive', action='store_true')\n",
    "    parser.add_argument('-l', '--log_file', nargs='?', default='')\n",
    "    parser.add_argument('-v', '--validate', action='store_true')\n",
    "\n",
    "    # Sequence annotation arguments\n",
    "    parser.add_argument('-f', '--feature_files', nargs='*', default=[])\n",
    "    parser.add_argument('-M', '--min_feature_length', nargs='?', default=40)\n",
    "    parser.add_argument('-na', '--no_annotation', action='store_true')\n",
    "    parser.add_argument('-e', '--extend_features', action='store_true')\n",
    "    parser.add_argument('-xs', '--extension_suffix', nargs='?', default='')\n",
    "    parser.add_argument('-x', '--extension_threshold', nargs='?', default=0.05)\n",
    "    parser.add_argument('-cm', '--complete_matches', action='store_true')\n",
    "    parser.add_argument('-sp', '--strip_prefixes', nargs='*', default=[])\n",
    "\n",
    "    # Annotation pruning arguments\n",
    "    parser.add_argument('-c', '--cover_offset', nargs='?', default=14)\n",
    "    parser.add_argument('-r', '--deletion_roles', nargs='*', default=[])\n",
    "    parser.add_argument('-d', '--delete_flat', action='store_true')\n",
    "    parser.add_argument('-np', '--no_pruning', action='store_true')\n",
    "    parser.add_argument('-a', '--auto_swap', action='store_true')\n",
    "    \n",
    "    parser.add_argument('-U', '--sbh_URL', nargs='?', default=None)\n",
    "    parser.add_argument('-u', '--username', nargs='?', default=None)\n",
    "    parser.add_argument('-w', '--password', nargs='?', default=None)\n",
    "    parser.add_argument('-F', '--feature_URLs', nargs='*', default=[])\n",
    "    parser.add_argument('-T', '--target_URLs', nargs='*', default=[])\n",
    "    \n",
    "    args = parser.parse_args(args)\n",
    "\n",
    "    logger = logging.getLogger('synbict')\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.propagate = False\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s ; %(levelname)s ; %(message)s')\n",
    "\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    if len(args.log_file) > 0:\n",
    "        file_handler = logging.FileHandler(args.log_file, \"w\")\n",
    "        file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "        file_handler.setFormatter(formatter)\n",
    "\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    sbol2.setHomespace(args.namespace)\n",
    "    sbol2.Config.setOption('validate', args.validate)\n",
    "    sbol2.Config.setOption('sbol_typed_uris', False)\n",
    "\n",
    "    sbh_arg_types = []\n",
    "\n",
    "    if args.username:\n",
    "        sbh_arg_types.append('username')\n",
    "\n",
    "    if args.password:\n",
    "        sbh_arg_types.append('password')\n",
    "\n",
    "    if args.sbh_URL:\n",
    "        synbiohub = sbol2.PartShop(args.sbh_URL)\n",
    "\n",
    "        if len(sbh_arg_types) == 2:\n",
    "            try:\n",
    "                synbiohub.login(args.username, args.password)\n",
    "            except SBOLError as exc:\n",
    "                if exc.error_code() == sbol2.SBOLErrorCode.SBOL_ERROR_BAD_HTTP_REQUEST:\n",
    "                    logger.warning('Unable to log into SynBioHub instance with URL %s', args.sbh_URL)\n",
    "        elif len(sbh_arg_types) == 1:\n",
    "            logger.warning('SynBioHub %s was provided but %s is missing.',\n",
    "                           sbh_arg_types[0],\n",
    "                           {'username', 'password'}.difference(sbh_arg_types).pop())\n",
    "\n",
    "    else:\n",
    "        synbiohub = None\n",
    "\n",
    "        if len(sbh_arg_types) > 0:\n",
    "            logger.warning('SynBioHub %s were provided but a SynBioHub instance URL is missing.',\n",
    "                           ', '.join(sbh_arg_types))\n",
    "\n",
    "    target_files = []\n",
    "\n",
    "    for target_file in args.target_files:\n",
    "        if os.path.isdir(target_file):\n",
    "            target_files.extend([os.path.join(target_file, tf) for tf in os.listdir(target_file) if\n",
    "                                 os.path.isfile(os.path.join(target_file, tf)) and (tf.endswith('.xml') or\n",
    "                                                                                    tf.endswith('.sbol') or\n",
    "                                                                                    tf.endswith('.gb') or\n",
    "                                                                                    tf.endswith('.genbank') or\n",
    "                                                                                    tf.endswith('.fasta') or\n",
    "                                                                                    tf.endswith('.faa') or\n",
    "                                                                                    tf.endswith('.fa') or\n",
    "                                                                                    tf.endswith('.fas') or\n",
    "                                                                                    tf.endswith('.fsa'))])\n",
    "        else:\n",
    "            target_files.append(target_file)\n",
    "\n",
    "    output_files = []\n",
    "\n",
    "    for i in range(0, len(target_files)):\n",
    "        if len(args.output_files) == 1 and os.path.isdir(args.output_files[0]):\n",
    "            (target_file_path, target_filename) = os.path.split(target_files[i])\n",
    "            (target_file_base, target_file_extension) = os.path.splitext(target_filename)\n",
    "\n",
    "            if len(args.output_suffix) > 0:\n",
    "                output_files.append(os.path.join(args.output_files[0], '_'.join([target_file_base, args.output_suffix + '.xml'])))\n",
    "            else:\n",
    "                output_files.append(os.path.join(args.output_files[0], target_file_base + '.xml'))\n",
    "        elif i < len(args.output_files):\n",
    "            output_files.append(args.output_files[i])\n",
    "        else:\n",
    "            (target_file_base, target_file_extension) = os.path.splitext(target_files[i])\n",
    "\n",
    "            if len(args.output_suffix) > 0:\n",
    "                output_files.append('_'.join([target_file_base, args.output_suffix + '.xml']))\n",
    "            else:\n",
    "                output_files.append(target_file_base + '.xml')\n",
    "\n",
    "    for i in range(0, len(args.target_URLs)):\n",
    "        if i + len(target_files) < len(args.output_files):\n",
    "            output_files.append(args.output_files[i])\n",
    "        else:\n",
    "            output_files.append('_'.join(['output', str(i + len(target_files))]) + '.xml')\n",
    "\n",
    "    feature_docs = []\n",
    "\n",
    "    for feature_file in args.feature_files:\n",
    "        feature_docs.append(load_sbol(feature_file))\n",
    "\n",
    "    if synbiohub:\n",
    "        for feature_URL in args.feature_URLs:\n",
    "            feature_doc = sbol2.Document()\n",
    "\n",
    "            try:\n",
    "                synbiohub.pull(feature_URL, feature_doc)\n",
    "\n",
    "                download_sequences(feature_doc, synbiohub)\n",
    "\n",
    "                feature_docs.append(feature_doc)\n",
    "            except NotFoundError as exc:\n",
    "                if is_sbol_not_found(exc):\n",
    "                    logger.warning('Unable to find feature URL %s at %s', feature_URL, sbh_URL)\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "    feature_library = FeatureLibrary(feature_docs)\n",
    "\n",
    "    if args.extend_features:\n",
    "        target_docs = []\n",
    "\n",
    "        for target_file in target_files:\n",
    "            target_docs.append(load_target_file(target_file))\n",
    "\n",
    "        if synbiohub:\n",
    "            for target_URL in args.target_URLs:\n",
    "                target_doc = sbol2.Document()\n",
    "\n",
    "                try:\n",
    "                    synbiohub.pull(target_URL, target_doc)\n",
    "\n",
    "                    download_sequences(target_doc, synbiohub)\n",
    "\n",
    "                    target_docs.append(target_doc)\n",
    "                except NotFoundError as exc:\n",
    "                    target_docs.append(None)\n",
    "\n",
    "                    if is_sbol_not_found(exc):\n",
    "                        logger.warning('Unable to find target URL %s at %s', target_URL, sbh_URL)\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "        filtered_output_files = [output_files[i] for i in range(0, len(target_docs)) if target_docs[i]]\n",
    "\n",
    "        target_docs = [target_docs[i] for i in range(0, len(target_docs)) if target_docs[i]]\n",
    "\n",
    "        target_library = FeatureLibrary(target_docs, False)\n",
    "\n",
    "        if args.minimal_output:\n",
    "            output_docs = [sbol2.Document() for i in range(0, len(target_library.docs))]\n",
    "        else:\n",
    "            output_docs = []\n",
    "\n",
    "        output_library = FeatureLibrary(output_docs, False)\n",
    "\n",
    "        curate(feature_library, target_library, output_library, filtered_output_files, args.extend_features,\n",
    "               args.no_annotation, int(args.min_feature_length), int(args.min_target_length),\n",
    "               float(args.extension_threshold), args.extension_suffix, args.in_place, args.minimal_output,\n",
    "               args.no_pruning, args.deletion_roles, int(args.cover_offset), args.delete_flat, args.auto_swap,\n",
    "               args.non_interactive, logger, args.complete_matches, args.strip_prefixes)\n",
    "    else:\n",
    "        for i in range(0, len(target_files)):\n",
    "            target_doc = load_target_file(target_files[i])\n",
    "\n",
    "            if target_doc:\n",
    "                target_library = FeatureLibrary([target_doc], False)\n",
    "\n",
    "                if args.minimal_output:\n",
    "                    output_docs = [sbol2.Document()]\n",
    "                else:\n",
    "                    output_docs = []\n",
    "\n",
    "                output_library = FeatureLibrary(output_docs, False)\n",
    "\n",
    "                curate(feature_library, target_library, output_library, [output_files[i]], args.extend_features,\n",
    "                       args.no_annotation, int(args.min_feature_length), int(args.min_target_length),\n",
    "                       float(args.extension_threshold), args.extension_suffix, args.in_place, args.minimal_output,\n",
    "                       args.no_pruning, args.deletion_roles, int(args.cover_offset), args.delete_flat, args.auto_swap,\n",
    "                       args.non_interactive, logger, args.complete_matches, args.strip_prefixes)\n",
    "\n",
    "        if synbiohub:\n",
    "            for target_URL in args.target_URLs:\n",
    "                try:\n",
    "                    target_doc = sbol2.Document()\n",
    "\n",
    "                    synbiohub.pull(target_URL, target_doc)\n",
    "\n",
    "                    download_sequences(target_doc, synbiohub)\n",
    "                except NotFoundError as exc:\n",
    "                    if is_sbol_not_found(exc):\n",
    "                        logger.warning('Unable to find target URL %s at %s', target_URL, sbh_URL)\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "                    target_doc = None\n",
    "\n",
    "                if target_doc:\n",
    "                    target_library = FeatureLibrary([target_doc], False)\n",
    "\n",
    "                    if args.minimal_output:\n",
    "                        output_docs = [sbol2.Document()]\n",
    "                    else:\n",
    "                        output_docs = []\n",
    "\n",
    "                    output_library = FeatureLibrary(output_docs, False)\n",
    "\n",
    "                    curate(feature_library, target_library, output_library, [output_files[i]], args.extend_features,\n",
    "                           args.no_annotation, int(args.min_feature_length), int(args.min_target_length),\n",
    "                           float(args.extension_threshold), args.extension_suffix, args.in_place, args.minimal_output,\n",
    "                           args.no_pruning, args.deletion_roles, int(args.cover_offset), args.delete_flat, args.auto_swap,\n",
    "                           args.non_interactive, logger, args.complete_matches, args.strip_prefixes)\n",
    "\n",
    "    logger.info('Finished curating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "feature_core.py - Core data structures for genetic features\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Set, Optional\n",
    "from Bio.Seq import Seq\n",
    "import logging\n",
    "import sbol2\n",
    "\n",
    "@dataclass\n",
    "class Feature:\n",
    "    \"\"\"\n",
    "    Represents a genetic feature with its sequence and metadata\n",
    "    \"\"\"\n",
    "    nucleotides: str\n",
    "    identity: str\n",
    "    roles: Set[str]\n",
    "    sub_identities: List[str] = field(default_factory=list)\n",
    "    parent_identities: List[str] = field(default_factory=list)\n",
    "    \n",
    "    # Standard SO terms for roles\n",
    "    SO_REGION: str = field(default='http://identifiers.org/so/SO:0000001', init=False)\n",
    "    SO_SEQUENCE_FEATURE: str = field(default='http://identifiers.org/so/SO:0000110', init=False)\n",
    "    GENERIC_ROLES: Set[str] = field(default_factory=lambda: {\n",
    "        'http://identifiers.org/so/SO:0000001',\n",
    "        'http://identifiers.org/so/SO:0000110'\n",
    "    }, init=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Initialize logger after dataclass initialization\"\"\"\n",
    "        self.logger = logging.getLogger('synbict.feature')\n",
    "        self.roles = set(self.roles)  # Ensure roles is a set\n",
    "\n",
    "    def reverse_complement_nucleotides(self) -> str:\n",
    "        \"\"\"Get the reverse complement of the feature's nucleotide sequence\"\"\"\n",
    "        return str(Seq(self.nucleotides).reverse_complement())\n",
    "\n",
    "    @classmethod\n",
    "    def has_non_generic_role(cls, roles: Set[str]) -> bool:\n",
    "        \"\"\"Check if the given roles contain any non-generic roles\"\"\"\n",
    "        return bool(roles.difference(cls.GENERIC_ROLES))\n",
    "\n",
    "    def is_non_generic(self) -> bool:\n",
    "        \"\"\"Check if this feature has any non-generic roles\"\"\"\n",
    "        return self.has_non_generic_role(self.roles)\n",
    "\n",
    "@dataclass\n",
    "class FeatureLibrary:\n",
    "    \"\"\"\n",
    "    A collection of features with indexing and lookup capabilities\n",
    "    \"\"\"\n",
    "    docs: List[sbol2.Document]\n",
    "    require_sequence: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Initialize library data structures\"\"\"\n",
    "        self.features: List[Feature] = []\n",
    "        self.updated_indices: Set[int] = set()\n",
    "        self.feature_map: dict = {}  # Maps feature ID to document index\n",
    "        self.feature_dict: dict = {}  # Maps feature ID to Feature object\n",
    "        self.name_to_idents: dict = {}  # Maps feature names to IDs\n",
    "        \n",
    "        self.logger = logging.getLogger('synbict.library')\n",
    "        self.logger.info('Loading features')\n",
    "        \n",
    "        # Load features from all documents\n",
    "        for i, doc in enumerate(self.docs):\n",
    "            self._load_features(doc, i, self.require_sequence)\n",
    "\n",
    "    def update(self, require_sequence: bool = True) -> List[Feature]:\n",
    "        \"\"\"\n",
    "        Update library with any new features from documents\n",
    "        \n",
    "        Returns:\n",
    "            List[Feature]: Newly added features\n",
    "        \"\"\"\n",
    "        added_features = []\n",
    "        for i, doc in enumerate(self.docs):\n",
    "            new_features = self._load_features(doc, i, require_sequence)\n",
    "            added_features.extend(new_features)\n",
    "            if new_features:\n",
    "                self.updated_indices.add(i)\n",
    "        return added_features\n",
    "\n",
    "    def get_updated_documents(self) -> List[sbol2.Document]:\n",
    "        \"\"\"Get all documents that have been updated\"\"\"\n",
    "        return [self.docs[i] for i in self.updated_indices]\n",
    "\n",
    "    def get_non_updated_indices(self) -> List[int]:\n",
    "        \"\"\"Get indices of documents that haven't been updated\"\"\"\n",
    "        return [i for i in range(len(self.docs)) if i not in self.updated_indices]\n",
    "\n",
    "    def _load_features(self, doc: sbol2.Document, doc_index: int, \n",
    "                      require_sequence: bool = True) -> List[Feature]:\n",
    "        \"\"\"\n",
    "        Load features from a single document\n",
    "        \n",
    "        Args:\n",
    "            doc: SBOL document to load from\n",
    "            doc_index: Index of document in library\n",
    "            require_sequence: Whether features must have sequence data\n",
    "            \n",
    "        Returns:\n",
    "            List[Feature]: Newly loaded features\n",
    "        \"\"\"\n",
    "        loaded_features = []\n",
    "        comp_seq_identities = set()\n",
    "\n",
    "        # Process component definitions\n",
    "        for comp_definition in doc.componentDefinitions:\n",
    "            if sbol2.BIOPAX_DNA not in comp_definition.types:\n",
    "                continue\n",
    "                \n",
    "            dna_seqs = self.get_DNA_sequences(comp_definition, doc)\n",
    "            for dna_seq in dna_seqs:\n",
    "                comp_seq_identities.add(dna_seq.identity)\n",
    "\n",
    "            # Skip if already loaded\n",
    "            if comp_definition.identity in self.feature_map:\n",
    "                continue\n",
    "\n",
    "            # Get sub-components\n",
    "            sub_identities = [sub_comp.definition \n",
    "                            for sub_comp in comp_definition.components]\n",
    "\n",
    "            # Create feature if it has sequence or sequence not required\n",
    "            if dna_seqs and len(dna_seqs) > 0:\n",
    "                feature = Feature(\n",
    "                    nucleotides=dna_seqs[0].elements,\n",
    "                    identity=comp_definition.identity,\n",
    "                    roles=set(comp_definition.roles),\n",
    "                    sub_identities=sub_identities,\n",
    "                    parent_identities=list(comp_definition.wasDerivedFrom)\n",
    "                )\n",
    "                self._add_feature(feature, doc_index, comp_definition)\n",
    "                loaded_features.append(feature)\n",
    "                \n",
    "            elif not require_sequence:\n",
    "                feature = Feature(\n",
    "                    nucleotides='',\n",
    "                    identity=comp_definition.identity,\n",
    "                    roles=set(comp_definition.roles),\n",
    "                    sub_identities=sub_identities,\n",
    "                    parent_identities=list(comp_definition.wasDerivedFrom)\n",
    "                )\n",
    "                self._add_feature(feature, doc_index, comp_definition)\n",
    "                loaded_features.append(feature)\n",
    "            else:\n",
    "                self.logger.warning(\n",
    "                    f'{comp_definition.identity} not loaded since its DNA sequence was not found'\n",
    "                )\n",
    "\n",
    "        # Process standalone sequences\n",
    "        loaded_features.extend(\n",
    "            self._load_standalone_sequences(doc, doc_index, comp_seq_identities)\n",
    "        )\n",
    "\n",
    "        return loaded_features\n",
    "\n",
    "    def _add_feature(self, feature: Feature, doc_index: int, \n",
    "                    comp_definition: sbol2.ComponentDefinition):\n",
    "        \"\"\"Add a feature to the library with proper indexing\"\"\"\n",
    "        self.features.append(feature)\n",
    "        self.feature_map[feature.identity] = doc_index\n",
    "        self.feature_dict[feature.identity] = feature\n",
    "        \n",
    "        if comp_definition.name:\n",
    "            if comp_definition.name not in self.name_to_idents:\n",
    "                self.name_to_idents[comp_definition.name] = []\n",
    "            self.name_to_idents[comp_definition.name].append(feature.identity)\n",
    "\n",
    "    def _load_standalone_sequences(self, doc: sbol2.Document, doc_index: int,\n",
    "                                 comp_seq_identities: Set[str]) -> List[Feature]:\n",
    "        \"\"\"Load sequences that aren't associated with components\"\"\"\n",
    "        loaded_features = []\n",
    "        \n",
    "        for seq in doc.sequences:\n",
    "            if (seq.identity not in comp_seq_identities and \n",
    "                seq.encoding == sbol2.SBOL_ENCODING_IUPAC):\n",
    "                \n",
    "                # Create component for sequence\n",
    "                seq_comp_def = sbol2.ComponentDefinition(\n",
    "                    f\"{seq.displayId}_comp\", \n",
    "                    sbol2.BIOPAX_DNA, \n",
    "                    '1'\n",
    "                )\n",
    "                seq_comp_def.sequences = [seq.identity]\n",
    "\n",
    "                try:\n",
    "                    doc.addComponentDefinition(seq_comp_def)\n",
    "                    feature = Feature(\n",
    "                        nucleotides=seq.elements,\n",
    "                        identity=seq_comp_def.identity,\n",
    "                        roles=set(),\n",
    "                        sub_identities=[],\n",
    "                        parent_identities=[]\n",
    "                    )\n",
    "                    self._add_feature(feature, doc_index, seq_comp_def)\n",
    "                    loaded_features.append(feature)\n",
    "                    \n",
    "                except (RuntimeError, sbol2.SBOLError) as e:\n",
    "                    self.logger.warning(\n",
    "                        f'Component could not be generated for DNA sequence {seq.identity}: {str(e)}'\n",
    "                    )\n",
    "\n",
    "        return loaded_features\n",
    "\n",
    "    @staticmethod\n",
    "    def get_DNA_sequences(comp_definition: sbol2.ComponentDefinition,\n",
    "                         doc: sbol2.Document) -> List[sbol2.Sequence]:\n",
    "        \"\"\"Get DNA sequences associated with a component definition\"\"\"\n",
    "        dna_seqs = []\n",
    "        \n",
    "        for seq_URI in comp_definition.sequences:\n",
    "            try:\n",
    "                seq = doc.getSequence(seq_URI)\n",
    "                if seq and seq.encoding == sbol2.SBOL_ENCODING_IUPAC:\n",
    "                    dna_seqs.append(seq)\n",
    "            except (RuntimeError, sbol2.SBOLError):\n",
    "                continue\n",
    "                \n",
    "        return dna_seqs\n",
    "\n",
    "    def get_features(self, min_feature_length: int = 0, \n",
    "                    children_only: bool = False) -> List[Feature]:\n",
    "        \"\"\"\n",
    "        Get features matching specified criteria\n",
    "        \n",
    "        Args:\n",
    "            min_feature_length: Minimum sequence length (0 for no minimum)\n",
    "            children_only: Only return features without parents\n",
    "            \n",
    "        Returns:\n",
    "            List[Feature]: Matching features\n",
    "        \"\"\"\n",
    "        if children_only:\n",
    "            parent_identities = {\n",
    "                parent_id \n",
    "                for feature in self.features\n",
    "                for parent_id in feature.parent_identities\n",
    "            }\n",
    "            return [\n",
    "                feature for feature in self.features\n",
    "                if ((min_feature_length == 0 or \n",
    "                     len(feature.nucleotides) > min_feature_length) and\n",
    "                    feature.identity not in parent_identities)\n",
    "            ]\n",
    "        else:\n",
    "            return [\n",
    "                feature for feature in self.features\n",
    "                if min_feature_length == 0 or \n",
    "                   len(feature.nucleotides) > min_feature_length\n",
    "            ]\n",
    "\n",
    "    def has_feature(self, identity: str) -> bool:\n",
    "        \"\"\"Check if feature exists in library\"\"\"\n",
    "        return identity in self.feature_map\n",
    "\n",
    "    def get_feature(self, identity: str) -> Optional[Feature]:\n",
    "        \"\"\"Get feature by identity\"\"\"\n",
    "        return self.feature_dict.get(identity)\n",
    "\n",
    "    def get_document(self, identity: str) -> Optional[sbol2.Document]:\n",
    "        \"\"\"Get document containing feature\"\"\"\n",
    "        index = self.get_document_index(identity)\n",
    "        return self.docs[index] if index >= 0 else None\n",
    "\n",
    "    def get_document_index(self, identity: str) -> int:\n",
    "        \"\"\"Get index of document containing feature\"\"\"\n",
    "        return self.feature_map.get(identity, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_get_features (__main__.TestFeatureLibrary.test_get_features)\n",
      "Test feature retrieval with length filtering ... ok\n",
      "test_load_features (__main__.TestFeatureLibrary.test_load_features)\n",
      "Test that features are loaded correctly from SBOL document ... ok\n",
      "test_annotate_features (__main__.TestFeatureCurator.test_annotate_features)\n",
      "Test feature annotation process ... ok\n",
      "test_feature_matching (__main__.TestFeatureAnnotater.test_feature_matching)\n",
      "Test feature matching functionality ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.012s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=4 errors=0 failures=0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "from unittest.mock import Mock, patch\n",
    "from Bio.Seq import Seq\n",
    "import sbol2\n",
    "\n",
    "class TestFeatureLibrary(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        # Create a temporary SBOL document\n",
    "        self.doc = sbol2.Document()\n",
    "        \n",
    "        # Create a test component definition\n",
    "        self.comp_def = sbol2.ComponentDefinition(\"test_comp\", sbol2.BIOPAX_DNA, '1')\n",
    "        self.comp_def.sequence = sbol2.Sequence(\"test_seq\", \"ATCG\", sbol2.SBOL_ENCODING_IUPAC, '1')\n",
    "        self.doc.addComponentDefinition(self.comp_def)\n",
    "        \n",
    "        # Initialize feature library\n",
    "        self.feature_lib = FeatureLibrary([self.doc])\n",
    "\n",
    "    def test_load_features(self):\n",
    "        \"\"\"Test that features are loaded correctly from SBOL document\"\"\"\n",
    "        self.assertEqual(len(self.feature_lib.features), 1)\n",
    "        self.assertEqual(self.feature_lib.features[0].nucleotides, \"ATCG\")\n",
    "        self.assertEqual(self.feature_lib.features[0].identity, self.comp_def.identity)\n",
    "\n",
    "    def test_get_features(self):\n",
    "        \"\"\"Test feature retrieval with length filtering\"\"\"\n",
    "        # Should return 1 feature (no minimum length)\n",
    "        self.assertEqual(len(self.feature_lib.get_features(min_feature_length=0)), 1)\n",
    "        \n",
    "        # Should return 0 features (minimum length > sequence length)\n",
    "        self.assertEqual(len(self.feature_lib.get_features(min_feature_length=5)), 0)\n",
    "\n",
    "class TestFeatureCurator(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        # Create mock libraries with proper configuration\n",
    "        self.target_lib = Mock()\n",
    "        self.output_lib = Mock()\n",
    "        \n",
    "        # Configure mocks\n",
    "        self.target_lib.docs = []  # Empty list of docs\n",
    "        self.output_lib.docs = []  # Empty list of docs\n",
    "        \n",
    "        # Create curator\n",
    "        self.curator = FeatureCurator(self.target_lib, self.output_lib)\n",
    "\n",
    "    def test_annotate_features(self):\n",
    "        \"\"\"Test feature annotation process\"\"\"\n",
    "        # Set up mock annotater\n",
    "        feature_annotater = Mock()\n",
    "        feature_annotater.annotate.return_value = [\"feature1\"]\n",
    "        \n",
    "        # Configure mock responses\n",
    "        mock_feature = Mock()\n",
    "        mock_feature.identity = \"feature1\"\n",
    "        self.target_lib.update.return_value = [mock_feature]\n",
    "        \n",
    "        # Test annotation\n",
    "        annotated, annotating = self.curator.annotate_features(\n",
    "            feature_annotater, \n",
    "            min_target_length=10,\n",
    "            in_place=False\n",
    "        )\n",
    "        \n",
    "        # Verify results\n",
    "        self.assertEqual(len(annotated), 1)\n",
    "        self.assertEqual(len(annotating), 0)\n",
    "        \n",
    "        # Verify annotater was called\n",
    "        feature_annotater.annotate.assert_called_once()\n",
    "\n",
    "class TestFeatureAnnotater(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        # Create test feature library\n",
    "        doc = sbol2.Document()\n",
    "        comp_def = sbol2.ComponentDefinition(\"test_feat\", sbol2.BIOPAX_DNA, '1')\n",
    "        comp_def.sequence = sbol2.Sequence(\"test_seq\", \"ATCG\", sbol2.SBOL_ENCODING_IUPAC, '1')\n",
    "        doc.addComponentDefinition(comp_def)\n",
    "        \n",
    "        self.feature_lib = FeatureLibrary([doc])\n",
    "        self.annotater = FeatureAnnotater(self.feature_lib, min_feature_length=0)\n",
    "\n",
    "    def test_feature_matching(self):\n",
    "        \"\"\"Test feature matching functionality\"\"\"\n",
    "        # Test exact match\n",
    "        matches = self.annotater.feature_matcher.extract_keywords(\"A T C G\", span_info=True)\n",
    "        self.assertEqual(len(matches), 1)\n",
    "\n",
    "        # Test no match\n",
    "        matches = self.annotater.feature_matcher.extract_keywords(\"G C T A\", span_info=True)\n",
    "        self.assertEqual(len(matches), 0)\n",
    "\n",
    "# Create test suite\n",
    "def create_test_suite():\n",
    "    suite = unittest.TestSuite()\n",
    "    suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestFeatureLibrary))\n",
    "    suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestFeatureCurator))\n",
    "    suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestFeatureAnnotater))\n",
    "    return suite\n",
    "\n",
    "# Run tests\n",
    "runner = unittest.TextTestRunner(verbosity=2)\n",
    "test_suite = create_test_suite()\n",
    "runner.run(test_suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
